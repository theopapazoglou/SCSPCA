{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp  # Original NumPy for data generation\n",
    "import autograd.numpy as np  # Autograd NumPy for manifold optimization\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import linalg\n",
    "from numpy.linalg import inv, det\n",
    "#!pip install pymanopt\n",
    "from pymanopt.manifolds import Grassmann, Euclidean, Product\n",
    "from pymanopt import Problem\n",
    "from pymanopt.optimizers import SteepestDescent\n",
    "import pymanopt.function\n",
    "from scipy.linalg import svd, qr, pinv, eigh\n",
    "import time\n",
    "from scipy.linalg import lu_factor, lu_solve\n",
    "from scipy.sparse.linalg import cg\n",
    "from scipy.sparse.linalg import svds, aslinearoperator, LinearOperator\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "def delta_kernel(Y):\n",
    "    n = len(Y)\n",
    "    K = onp.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            K[i, j] = 1 if Y[i] == Y[j] else 0\n",
    "    return K\n",
    "\n",
    "\n",
    "def barshan_pca(X, Y, K, q, epsilon= 1e-6):\n",
    "    # Cholesky decomposition of K\n",
    "    n = K.shape[0]\n",
    "    K_reg = K + epsilon * np.eye(n)\n",
    "    Delta = linalg.cholesky(K_reg, lower=True)  # K = Delta @ Delta.T\n",
    "    psi = X.T @ Delta  # Shape: (p x n)\n",
    "    # SVD of psi\n",
    "    U, s, Vt = svd(psi, full_matrices=False)\n",
    "    Sigma = onp.diag(s)\n",
    "    # Extract top q components\n",
    "    U_d = U[:, :q]\n",
    "    Sigma_d = Sigma[:q, :q]\n",
    "    V_d = Vt.T[:, :q]  # Vt is (n x p), so V_d is (p x q)\n",
    "    # Compute U_hat\n",
    "    U_hat = psi @ V_d @ linalg.inv(Sigma_d)  # Shape: (p x q)\n",
    "    eigenvalues = s[:q]**2  # Singular values squared\n",
    "    return {'W': U_hat, 'singular_values': eigenvalues}\n",
    "\n",
    "\n",
    "\n",
    "# Supervised PCA (Updated)\n",
    "def supervised_pca(X, Y, q, p, m, lambda_):\n",
    "    XtX = X.T @ X\n",
    "    C = X.T @ delta_kernel(Y) @ X + lambda_ * XtX\n",
    "    # Subsample m features\n",
    "    indices = onp.random.choice(p, size=m, replace=False)\n",
    "    C_nm = C[:, indices]  # Shape: (p x m)\n",
    "    C_mm = C[indices, :][:, indices]  # Shape: (m x m)\n",
    "    # Eigen decomposition of C_mm\n",
    "    eigenvalues, U_m = linalg.eigh(C_mm)  # Symmetric matrix\n",
    "    sorted_indices = onp.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    U_m = U_m[:, sorted_indices]\n",
    "    # Compute Lambda_m and its inverse square root\n",
    "    Lambda_m = onp.diag(eigenvalues)\n",
    "    Lambda_m_inv_sqrt = onp.diag(1.0 / onp.sqrt(onp.maximum(eigenvalues, 1e-6)))\n",
    "    # Compute U_n\n",
    "    U_n = C_nm @ U_m @ Lambda_m_inv_sqrt  # Shape: (p x m)\n",
    "    # Compute approximate projection\n",
    "    C_mm_inv = pinv(C_mm)  # Generalized inverse\n",
    "    C_approx = C_nm @ C_mm_inv @ C_nm.T\n",
    "    # Extract top q components and orthogonalize\n",
    "    W = U_n[:, :q]\n",
    "    W, _ = qr(W, mode='economic')  # Orthogonalize W\n",
    "    return {'W': W, 'eigenvalues': eigenvalues[:q]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def proximal_l1(b, lambda_, q):\n",
    "    Z = np.sign(b) * np.maximum(np.abs(b) - lambda_, 0)\n",
    "    act_set = (Z != 0).astype(int)\n",
    "    inact_set = 1 - act_set\n",
    "    return Z, act_set, inact_set\n",
    "\n",
    "#The following function generates a duplication matrix Dn and its pseudo-inverse pDn to be used later in the RSSN method.\n",
    "def duplication_matrix(q):\n",
    "    n = q * (q + 1) // 2\n",
    "    Dn = np.zeros((q * q, n))\n",
    "    idx = 0\n",
    "    for i in range(q):\n",
    "        for j in range(i, q):\n",
    "            Dn[i * q + j, idx] = 1\n",
    "            if i != j:\n",
    "                Dn[j * q + i, idx] = 1\n",
    "            idx += 1\n",
    "    pDn = np.linalg.pinv(Dn)\n",
    "    return Dn, pDn\n",
    "\n",
    "#Applies the Jacobian of the residual equation\n",
    "def linop(Blkd, x, q, t, reg):\n",
    "    V = np.zeros_like(x)\n",
    "    for i in range(q):\n",
    "        V[:, i] = Blkd[i] @ x[:, i]\n",
    "    return 2 * t * (V + V.T) + reg * x\n",
    "\n",
    "#The following function performs the optimization using the semi-smooth newton algorithm\n",
    "def semi_newton_matrix(n, q, W, t, C, mut, inner_tol, inner_max_iter, Lam0, Dn, pDn):\n",
    "    WtW = np.eye(q)\n",
    "    Wt = W.T\n",
    "    stop_flag = 0\n",
    "    Lam = Lam0.copy()\n",
    "    W_Lam_prod = C + 2 * t * (W @ Lam)\n",
    "    Z, Act_set, Inact_set = proximal_l1(W_Lam_prod, mut, q)\n",
    "    ZW = Z.T @ W\n",
    "    R_Lam = ZW + ZW.T - 2 * np.eye(q)\n",
    "    RE = pDn @ R_Lam.flatten()\n",
    "    r_l = np.linalg.norm(R_Lam, 'fro')\n",
    "\n",
    "    lambda_ = 0.2\n",
    "    j = 0\n",
    "\n",
    "    while r_l**2 > inner_tol:\n",
    "        reg = lambda_ * max(min(r_l, 0.1), 1e-11)\n",
    "        nnzZ = np.count_nonzero(Z)\n",
    "\n",
    "        if q < 15:\n",
    "            if nnzZ > q * (q + 1) // 2:\n",
    "                g = np.zeros((q * q, q * q))\n",
    "                for i in range(q):\n",
    "                    g[i*q:(i+1)*q, i*q:(i+1)*q] = Wt @ (Act_set[:, i, None] * W)\n",
    "                G = 4 * t * (pDn @ (g @ Dn))\n",
    "                lu, piv = lu_factor(G + reg * np.eye(q * (q + 1) // 2))\n",
    "                new_d = -lu_solve((lu, piv), RE)\n",
    "            else:\n",
    "                Wstack = np.zeros((nnzZ, q * q))\n",
    "                dim = 0\n",
    "                for i in range(q):\n",
    "                    row = np.where(Act_set[:, i])[0]\n",
    "                    Wstack[dim:dim+len(row), i*q:(i+1)*q] = W[row, :]\n",
    "                    dim += len(row)\n",
    "                V = Wstack @ Dn\n",
    "                U = 4 * t * (pDn @ Wstack.T)\n",
    "                lu, piv = lu_factor(np.eye(nnzZ) + (1/reg) * (V @ U))\n",
    "                new_d = -(1/reg * RE - (1/reg**2) * U @ lu_solve((lu, piv), V @ RE))\n",
    "            new_d = Dn @ new_d\n",
    "            new_d = new_d.reshape(q, q)\n",
    "        else:\n",
    "            Blkd = [None] * q\n",
    "            for i in range(q):\n",
    "                ind = Act_set[:, i].astype(bool)\n",
    "                if np.sum(ind) < n / 2:\n",
    "                    W_ind = W[ind, :]\n",
    "                    Blkd[i] = W_ind.T @ W_ind\n",
    "                else:\n",
    "                    ind = Inact_set[:, i].astype(bool)\n",
    "                    W_ind = Wt[:, ind]\n",
    "                    Blkd[i] = WtW - W_ind @ W_ind.T\n",
    "            new_d, _ = cg(lambda x: linop(Blkd, x, q, t, reg), -R_Lam, tol=min(1e-4, 1e-3 * r_l))\n",
    "            new_d = new_d.reshape(q, q)\n",
    "\n",
    "        t_new = 1.0\n",
    "        W_d_prod = 2 * t * (W @ new_d)\n",
    "        W_Lam_new_prod = W_Lam_prod + t_new * W_d_prod\n",
    "        Z, Act_set, Inact_set = proximal_l1(W_Lam_new_prod, mut, q)\n",
    "        ZW = Z.T @ W\n",
    "        R_Lam_new = ZW + ZW.T - 2 * np.eye(q)\n",
    "        r_l_new = np.linalg.norm(R_Lam_new, 'fro')\n",
    "\n",
    "        while r_l_new**2 >= r_l**2 * (1 - 0.001 * t_new) and t_new > 1e-3:\n",
    "            t_new *= 0.5\n",
    "            W_Lam_new_prod = W_Lam_prod + t_new * W_d_prod\n",
    "            Z, Act_set, Inact_set = proximal_l1(W_Lam_new_prod, mut, q)\n",
    "            ZW = Z.T @ W\n",
    "            R_Lam_new = ZW + ZW.T - 2 * np.eye(q)\n",
    "            r_l_new = np.linalg.norm(R_Lam_new, 'fro')\n",
    "\n",
    "        Lam += t_new * new_d\n",
    "        r_l = r_l_new\n",
    "        R_Lam = R_Lam_new\n",
    "        RE = pDn @ R_Lam.flatten()\n",
    "        W_Lam_prod = W_Lam_new_prod\n",
    "\n",
    "        if j > inner_max_iter:\n",
    "            stop_flag = 1\n",
    "            break\n",
    "        j += 1\n",
    "\n",
    "    return Z, j, Lam, r_l, stop_flag\n",
    "\n",
    "#This is the ManPG algorithm adapted specifically for sparse CSPCA. The projection matrix W is initialised based on the standard CSPCA method.\n",
    "def manpg_orth_sparse(C, q, n, eta, maxiter=1000, tol=1e-6, inner_iter=100, phi_init=None):\n",
    "    start_time = time.time()\n",
    "\n",
    "    #eta corresponds to the sparsity parameter, phi is defined as the solution to the standard CSPCA problem.\n",
    "    eta = eta.reshape(-1, 1)\n",
    "    if phi_init is None:\n",
    "      eigenvalues, eigenvectors = eigh(C)\n",
    "      phi_init = eigenvectors[:, -q:]\n",
    "\n",
    "    Dn, pDn = duplication_matrix(q)\n",
    "    L = 2 * abs(max(eigenvalues))\n",
    "    t = 2 / L\n",
    "    t_min = 1e-4\n",
    "    #tol = tol * n * q increase the tolerance based on the original paper!\n",
    "\n",
    "    # Initialization\n",
    "    W = phi_init.copy()\n",
    "    AW = C @ W\n",
    "    F = [-np.sum(W * AW) + np.sum(eta * np.abs(W))]\n",
    "    num_inner = np.zeros(maxiter)\n",
    "    num_linesearch = 0\n",
    "    num_inexact = 0\n",
    "    alpha = 1.0\n",
    "\n",
    "    for iter_ in range(1, maxiter):\n",
    "        ngx = 2 * AW\n",
    "        neg_pgx = ngx\n",
    "\n",
    "        if alpha < t_min or num_inexact > 10:\n",
    "            inner_tol = max(5e-16, min(1e-14, 1e-5 * tol * t**2))\n",
    "        else:\n",
    "            inner_tol = max(1e-13, min(1e-11, 1e-3 * tol * t**2))\n",
    "\n",
    "        Lam_init = np.zeros((q, q)) if iter_ == 1 else Lam\n",
    "        PY, num_inner[iter_], Lam, _, in_flag = semi_newton_matrix(\n",
    "            n, q, W, t, W + t * neg_pgx, eta * t, inner_tol, inner_iter, Lam_init, Dn, pDn\n",
    "        )\n",
    "        if in_flag:\n",
    "            num_inexact += 1\n",
    "\n",
    "        alpha = 1.0\n",
    "        D = PY - W\n",
    "        def orthonormalize_stable(PY, epsilon=1e-8):\n",
    "            M = PY.T @ PY + epsilon * np.eye(PY.shape[1])\n",
    "            U, Sigma, Vt = svd(M, full_matrices=False)\n",
    "            Sigma_inv_sqrt = np.diag([1 / np.sqrt(s) if s > epsilon else 0.0 for s in Sigma])\n",
    "            return PY @ (U @ Sigma_inv_sqrt @ Vt)\n",
    "\n",
    "        Z = orthonormalize_stable(PY)\n",
    "        #U, Sigma, Vt = svd(PY.T @ PY, full_matrices=False)\n",
    "        #Z = PY @ (U @ np.diag(np.sqrt(1 / Sigma)) @ Vt)\n",
    "        AZ = C @ Z\n",
    "        f_trial = -np.sum(Z * AZ)\n",
    "        F_trial = f_trial + np.sum(eta * np.abs(Z))\n",
    "        normDsquared = np.linalg.norm(D, 'fro')**2\n",
    "\n",
    "        if normDsquared / t**2 < tol:\n",
    "            break\n",
    "\n",
    "        while F_trial >= F[-1] - 0.5 / t * alpha * normDsquared:\n",
    "            alpha *= 0.5\n",
    "            num_linesearch += 1\n",
    "            if alpha < t_min:\n",
    "                num_inexact += 1\n",
    "                break\n",
    "            PY = W + alpha * D\n",
    "            Z = orthonormalize_stable(PY)\n",
    "            #U, Sigma, Vt = svd(PY.T @ PY, full_matrices=False)\n",
    "            #Z = PY @ (U @ np.diag(np.sqrt(1 / Sigma)) @ Vt)\n",
    "            AZ = C @ Z\n",
    "            f_trial = -np.sum(Z * AZ)\n",
    "            F_trial = f_trial + np.sum(eta * np.abs(Z))\n",
    "\n",
    "        W = Z\n",
    "        AW = AZ\n",
    "        F.append(F_trial)\n",
    "\n",
    "    #W[np.abs(W) <= 1e-4] = 0\n",
    "    W_manpg = W\n",
    "    time_manpg = time.time() - start_time\n",
    "    mean_ssn = np.sum(num_inner) / iter_\n",
    "\n",
    "    sparsity = np.count_nonzero(W_manpg == 0) / (n * q)\n",
    "    F_manpg = F[-1]\n",
    "    flag_succ = 1 if iter_ < maxiter - 1 else 0\n",
    "\n",
    "    print(f\"ManPG: Iter {iter_}, Fval {min(F):.5e}, CPU {time_manpg:.2f}, \"\n",
    "          f\"Sparsity {sparsity:.2f}, Inner Inexact {num_inexact}, \"\n",
    "          f\"Avg Inner {mean_ssn:.2f}, Total Linesearch {num_linesearch}\")\n",
    "\n",
    "    return W_manpg, F_manpg, sparsity, time_manpg, iter_, flag_succ, num_linesearch, mean_ssn\n",
    "\n",
    "# SPLS and SPCA functions (provided by user)\n",
    "def ust(b, eta):\n",
    "    b_ust = np.zeros_like(b)\n",
    "    if eta < 1:\n",
    "        valb = np.abs(b) - eta * np.max(np.abs(b))\n",
    "        mask = valb >= 0\n",
    "        b_ust[mask] = valb[mask] * np.sign(b)[mask]\n",
    "    return b_ust\n",
    "\n",
    "\n",
    "def rootmatrix(x):\n",
    "    eigvals, eigvecs = np.linalg.eigh(x)\n",
    "    eigvals = (eigvals + np.abs(eigvals)) / 2\n",
    "    sqrt_d = np.sqrt(eigvals)\n",
    "    return eigvecs @ np.diag(sqrt_d) @ eigvecs.T\n",
    "\n",
    "def convcheck(beta1, beta2):\n",
    "    a = np.max(np.abs(beta1 + beta2), axis=0)\n",
    "    b = np.max(np.abs(beta1 - beta2), axis=0)\n",
    "    d = len(a)\n",
    "    x = np.minimum(a, b)\n",
    "    return np.max(x)\n",
    "\n",
    "def updateRR(xnew, R=None, xold=None, lambda_=0, eps=1e-16):\n",
    "    xtx = (np.sum(xnew**2) + lambda_) / (1 + lambda_)\n",
    "    norm_xnew = np.sqrt(xtx)\n",
    "    if R is None:\n",
    "        R = np.array([[norm_xnew]])\n",
    "        return R, 1\n",
    "    Xtx = np.dot(xnew.T, xold) / (1 + lambda_)\n",
    "    r = linalg.solve_triangular(R, Xtx, lower=False, trans='T')\n",
    "    rpp = norm_xnew**2 - np.sum(r**2)\n",
    "    if rpp <= eps:\n",
    "        rpp = eps\n",
    "        rank = R.shape[0]\n",
    "    else:\n",
    "        rpp = np.sqrt(rpp)\n",
    "        rank = R.shape[0] + 1\n",
    "    R_new = np.zeros((R.shape[0] + 1, R.shape[1] + 1))\n",
    "    R_new[:-1, :-1] = R\n",
    "    R_new[:-1, -1] = r\n",
    "    R_new[-1, -1] = rpp\n",
    "    return R_new, rank\n",
    "\n",
    "def solvebeta(x, y, paras, max_steps=None, sparse=\"penalty\", eps=1e-16):\n",
    "    lambda_ = paras[0]\n",
    "    penalty_param = paras[1]\n",
    "    n, m = x.shape\n",
    "    im = np.arange(m)\n",
    "    one = np.ones(n)\n",
    "    if lambda_ > 0:\n",
    "        maxvars = m\n",
    "    else:\n",
    "        maxvars = min(m, n - 1)\n",
    "        if m == n:\n",
    "            maxvars = m\n",
    "    if max_steps is None:\n",
    "        max_steps = 50 * maxvars\n",
    "    d1 = np.sqrt(lambda_)\n",
    "    d2 = 1 / np.sqrt(1 + lambda_)\n",
    "    Cvec = np.dot(y.T, x) * d2\n",
    "    ssy = np.sum(y**2)\n",
    "    residuals = np.concatenate([y, np.zeros(m)])\n",
    "    penalty = np.array([np.max(np.abs(Cvec))])\n",
    "    if sparse == \"penalty\" and penalty[0] * 2 / d2 <= penalty_param:\n",
    "        return np.zeros(m)\n",
    "    beta = np.zeros(m)\n",
    "    first_in = np.zeros(m, dtype=int)\n",
    "    active = []\n",
    "    ignores = []\n",
    "    actions = [None] * max_steps\n",
    "    Sign = []\n",
    "    R = None\n",
    "    k = 0\n",
    "    while k < max_steps and len(active) < maxvars - len(ignores):\n",
    "        action = []\n",
    "        k += 1\n",
    "        inactive = im if k == 1 else np.setdiff1d(im, active + ignores)\n",
    "        C = Cvec[inactive]\n",
    "        Cmax = np.max(np.abs(C))\n",
    "        new = np.abs(C) == Cmax\n",
    "        C = C[~new]\n",
    "        new = inactive[new]\n",
    "        for inew in new:\n",
    "            R, rank = updateRR(x[:, inew], R, x[:, active] if active else None, lambda_, eps)\n",
    "            if rank == len(active):\n",
    "                R = R[:len(active), :len(active)] if len(active) > 0 else np.array([[]])\n",
    "                ignores.append(inew)\n",
    "                action.append(-inew)\n",
    "            else:\n",
    "                if first_in[inew] == 0:\n",
    "                    first_in[inew] = k\n",
    "                active.append(inew)\n",
    "                Sign.append(np.sign(Cvec[inew]))\n",
    "                action.append(inew)\n",
    "        Gi1 = linalg.solve_triangular(R, linalg.solve_triangular(R, Sign, lower=False, trans='T'), lower=False)\n",
    "        A = 1 / np.sqrt(np.sum(Gi1 * Sign))\n",
    "        w = A * Gi1\n",
    "        u1 = np.dot(x[:, active], w) * d2\n",
    "        u2 = np.zeros(m)\n",
    "        u2[active] = d1 * d2 * w\n",
    "        u = np.concatenate([u1, u2])\n",
    "        if len(active) == maxvars - len(ignores):\n",
    "            gamhat = Cmax / A\n",
    "        else:\n",
    "            a = (np.dot(u1, x[:, np.setdiff1d(im, active + ignores)]) + d1 * u2[np.setdiff1d(im, active + ignores)]) * d2\n",
    "            gam = np.concatenate([(Cmax - C) / (A - a), (Cmax + C) / (A + a)])\n",
    "            gamhat = np.min(gam[gam > eps]) if len(gam) > 0 else Cmax / A\n",
    "        dropid = None\n",
    "        b1 = beta[active]\n",
    "        z1 = -b1 / w\n",
    "        zmin = np.min(z1[z1 > eps]) if np.any(z1 > eps) else gamhat\n",
    "        drops = False\n",
    "        if zmin < gamhat:\n",
    "            gamhat = zmin\n",
    "            drops = z1 == zmin\n",
    "        beta2 = beta.copy()\n",
    "        beta[active] += gamhat * w\n",
    "        residuals -= gamhat * u\n",
    "        Cvec = (np.dot(residuals[:n], x) + d1 * residuals[n:]) * d2\n",
    "        penalty = np.append(penalty, penalty[-1] - np.abs(gamhat * A))\n",
    "        if sparse == \"penalty\" and penalty[-1] * 2 / d2 <= penalty_param:\n",
    "            s1 = penalty[-1] * 2 / d2\n",
    "            s2 = penalty[-2] * 2 / d2 if len(penalty) > 1 else s1\n",
    "            beta = ((s2 - penalty_param) / (s2 - s1)) * beta + ((penalty_param - s1) / (s2 - s1)) * beta2\n",
    "            beta *= d2\n",
    "            break\n",
    "        if isinstance(drops, np.ndarray) and drops.any() or drops is True:  # Handle both array and boolean\n",
    "            dropid = np.where(drops)[0] if isinstance(drops, np.ndarray) else []\n",
    "            for id in reversed(dropid):\n",
    "                R = R[:-1, :-1] if R.shape[0] > 1 else np.array([[]])\n",
    "            dropid = np.array(active)[drops] if isinstance(drops, np.ndarray) else []\n",
    "            beta[dropid] = 0\n",
    "            active = [a for i, a in enumerate(active) if not drops[i]] if isinstance(drops, np.ndarray) else active\n",
    "            Sign = [s for i, s in enumerate(Sign) if not drops[i]] if isinstance(drops, np.ndarray) else Sign\n",
    "        if sparse == \"varnum\" and len(active) >= penalty_param:\n",
    "            break\n",
    "        actions[k-1] = action\n",
    "    return beta\n",
    "\n",
    "def spca(x, K, para, type_=\"predictor\", sparse=\"penalty\", use_corr=False, lambda_=1e-6, max_iter=200, trace=False, eps_conv=1e-3):\n",
    "    x = np.asarray(x)\n",
    "    n, p = x.shape if type_ == \"predictor\" else (None, x.shape[0])\n",
    "\n",
    "    if type_ == \"predictor\":\n",
    "        if n / p >= 100:\n",
    "            print(\"Consider using type='Gram' with the sample covariance/correlation matrix for efficiency.\")\n",
    "        mean_x = np.mean(x, axis=0)\n",
    "        scale_x = np.std(x, axis=0) if use_corr else np.ones(p)\n",
    "        if use_corr:\n",
    "            scale_x[scale_x == 0] = 1.0\n",
    "        x = (x - mean_x) / scale_x\n",
    "    elif type_ == \"Gram\":\n",
    "        x = rootmatrix(x)\n",
    "\n",
    "    U, s, Vt = linalg.svd(x, full_matrices=False)\n",
    "    v = Vt.T\n",
    "    total_variance = np.sum(s**2)\n",
    "    alpha = v[:, :K]\n",
    "    beta = alpha.copy()\n",
    "\n",
    "    for i in range(K):\n",
    "        y = np.dot(x, alpha[:, i])\n",
    "        beta[:, i] = solvebeta(x, y, paras=[lambda_, para[i]], sparse=sparse)\n",
    "\n",
    "    xtx = np.dot(x.T, x)\n",
    "    temp = beta.copy()\n",
    "    norm_temp = np.sqrt(np.sum(temp**2, axis=0))\n",
    "    norm_temp[norm_temp == 0] = 1\n",
    "    temp = temp / norm_temp\n",
    "\n",
    "    k = 0\n",
    "    diff = 1\n",
    "    while k < max_iter and diff > eps_conv:\n",
    "        k += 1\n",
    "        alpha = np.dot(xtx, beta)\n",
    "        U, _, Vt = linalg.svd(alpha, full_matrices=False)\n",
    "        alpha = np.dot(U, Vt)\n",
    "\n",
    "        for i in range(K):\n",
    "            y = np.dot(x, alpha[:, i])\n",
    "            beta[:, i] = solvebeta(x, y, paras=[lambda_, para[i]], sparse=sparse)\n",
    "\n",
    "        norm_beta = np.sqrt(np.sum(beta**2, axis=0))\n",
    "        norm_beta[norm_beta == 0] = 1\n",
    "        beta2 = beta / norm_beta\n",
    "        diff = convcheck(beta2, temp)\n",
    "        temp = beta2\n",
    "\n",
    "        if trace and k % 10 == 0:\n",
    "            print(f\"Iterations: {k}\")\n",
    "\n",
    "    norm_beta = np.sqrt(np.sum(beta**2, axis=0))\n",
    "    norm_beta[norm_beta == 0] = 1\n",
    "    beta = beta / norm_beta\n",
    "\n",
    "    u = np.dot(x, beta)\n",
    "    R = np.linalg.qr(u)[1]\n",
    "    pev = np.diag(R**2) / total_variance\n",
    "\n",
    "    result = {\n",
    "        \"type\": type_,\n",
    "        \"K\": K,\n",
    "        \"loadings\": beta,\n",
    "        \"pev\": pev,\n",
    "        \"var_all\": total_variance,\n",
    "        \"para\": para,\n",
    "        \"lambda\": lambda_\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "class SDSPCA:\n",
    "    def __init__(self, n_components=2, alpha=1.0, beta=1.0, max_iter=100, tol=1e-6):\n",
    "        self.n_components = n_components\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def _create_label_matrix(self, y):\n",
    "        \"\"\"Create the class indicator matrix B from labels y\"\"\"\n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(classes)\n",
    "        n_samples = len(y)\n",
    "        B = np.zeros((n_classes, n_samples))\n",
    "        \n",
    "        for i, cls in enumerate(classes):\n",
    "            B[i, y == cls] = 1\n",
    "            \n",
    "        return B\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data (scikit-learn convention).\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target labels.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Transpose X to match paper's (m × n) convention\n",
    "        X = X.T  # Now shape (n_features, n_samples)\n",
    "        \n",
    "        # Create label matrix B (c × n)\n",
    "        self.B_ = self._create_label_matrix(y)\n",
    "        n_classes = self.B_.shape[0]\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.Q_ = np.random.randn(n_samples, self.n_components)\n",
    "        self.Q_ = np.linalg.qr(self.Q_)[0]  # Orthogonalize\n",
    "        self.V_ = np.eye(n_samples)\n",
    "        self.A_ = np.random.randn(n_classes, self.n_components)\n",
    "        \n",
    "        prev_obj = np.inf\n",
    "        \n",
    "        for iter_ in range(self.max_iter):\n",
    "            # Update Y (m × k)\n",
    "            self.Y_ = X @ self.Q_\n",
    "            \n",
    "            # Update A (c × k)\n",
    "            self.A_ = self.B_ @ self.Q_\n",
    "            \n",
    "            # Update Q (n × k)\n",
    "            # Compute matrix Z = -XᵀX - αBᵀB + βV\n",
    "            XTX = X.T @ X  # (n × n)\n",
    "            BTB = self.B_.T @ self.B_  # (n × n)\n",
    "            Z = -XTX - self.alpha * BTB + self.beta * self.V_  # (n × n)\n",
    "            \n",
    "            # Get eigenvectors corresponding to smallest eigenvalues\n",
    "            eigvals, eigvecs = eigh(Z, subset_by_index=[0, self.n_components-1])\n",
    "            self.Q_ = eigvecs\n",
    "            \n",
    "            # Update V (n × n)\n",
    "            row_norms = np.linalg.norm(self.Q_, axis=1)\n",
    "            self.Q_[row_norms < 1e-5, :] = 0 \n",
    "            self.V_ = np.diag(1. / (2 * np.maximum(row_norms, 1e-16)))\n",
    "            \n",
    "            # Check convergence\n",
    "            current_obj = (\n",
    "                np.linalg.norm(X - self.Y_ @ self.Q_.T, 'fro')**2 +\n",
    "                self.alpha * np.linalg.norm(self.B_ - self.A_ @ self.Q_.T, 'fro')**2 +\n",
    "                self.beta * np.sum(row_norms)\n",
    "            )\n",
    "            \n",
    "            if np.abs(prev_obj - current_obj) < self.tol:\n",
    "                break\n",
    "                \n",
    "            prev_obj = current_obj\n",
    "            \n",
    "        self.n_iter_ = iter_ + 1\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to the principal components.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data to transform.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Q : array-like, shape (n_samples, n_components)\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "#        # Project using the learned Y (X is n_samples × m, Y is m × k)\n",
    "#        X = np.asarray(X)\n",
    "#        return X @ self.Q_\n",
    "        return X @ self.Y_\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"Fit the model and transform the data.\"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.Q_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def soft_threshold(a, tau):\n",
    "    return np.sign(a) * np.maximum(np.abs(a) - tau, 0)\n",
    "\n",
    "\n",
    "\n",
    "def find_tau(a, c, max_iter=5000, tol=1e-4):\n",
    "    # Check if tau = 0 satisfies the L1 constraint\n",
    "    v_temp = soft_threshold(a, 0)\n",
    "    if np.linalg.norm(v_temp) > 0:\n",
    "        v_temp = v_temp / np.linalg.norm(v_temp)\n",
    "        if np.sum(np.abs(v_temp)) <= c:\n",
    "            return 0\n",
    "\n",
    "    # Binary search for tau\n",
    "    tau_min, tau_max = 0, np.max(np.abs(a))\n",
    "    best_tau, best_norm_diff = 0, float('inf')\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        tau = (tau_min + tau_max) / 2\n",
    "        v_temp = soft_threshold(a, tau)\n",
    "        if np.linalg.norm(v_temp) > 0:\n",
    "            v_temp = v_temp / np.linalg.norm(v_temp)\n",
    "            l1_norm = np.sum(np.abs(v_temp))\n",
    "            norm_diff = abs(l1_norm - c)\n",
    "\n",
    "            if norm_diff < best_norm_diff:\n",
    "                best_tau, best_norm_diff = tau, norm_diff\n",
    "\n",
    "            if norm_diff < tol:\n",
    "                return tau\n",
    "\n",
    "            if l1_norm > c:\n",
    "                tau_min = tau\n",
    "            else:\n",
    "                tau_max = tau\n",
    "        else:\n",
    "            tau_min = tau  # If v_temp is zero, increase tau\n",
    "\n",
    "    print(f\"Warning: Tau search did not converge within tolerance (best norm diff: {best_norm_diff:.6f})\")\n",
    "    return best_tau\n",
    "\n",
    "\n",
    "def compute_kernel(Y, kernel_type='linear', sigma=1.0, epsilon=1e-6):\n",
    "    n = Y.shape[0]\n",
    "    Y = Y.reshape(-1, 1)  # Ensure Y is a column vector\n",
    "\n",
    "    if kernel_type == 'linear':\n",
    "        L = Y @ Y.T  # Linear kernel (for regression)\n",
    "    elif kernel_type == 'rbf':\n",
    "        pairwise_dists = np.sum(Y**2, axis=1).reshape(-1, 1) + np.sum(Y**2, axis=1) - 2 * (Y @ Y.T)\n",
    "        L = np.exp(-pairwise_dists / (2 * sigma**2))  # RBF kernel\n",
    "    elif kernel_type == 'delta':\n",
    "        # Delta kernel: 1 if Y_i == Y_j, else 0 (for classification)\n",
    "        L = (Y == Y.T).astype(float)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported kernel type: {kernel_type}\")\n",
    "\n",
    "    # Add ridge term for numerical stability\n",
    "    L += epsilon * np.eye(n)\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sspca(X, Y, K, c, X_test=None, Y_test=None, kernel_type='linear',\n",
    "          sigma=1.0, max_iter=1000, tol=1e-6, verbose=False, standardize=True):\n",
    "    \"\"\"\n",
    "    Sparse Supervised Principal Component Analysis (SSPCA) with prediction and RMSE.\n",
    "\n",
    "    Parameters:\n",
    "    X : ndarray\n",
    "        Input training data matrix (n x p)\n",
    "    Y : ndarray\n",
    "        Target variable (n x 1 or n for classification)\n",
    "    K : int\n",
    "        Number of components\n",
    "    c : float\n",
    "        L1 regularization parameter (1 <= c <= sqrt(p))\n",
    "    X_test : ndarray, optional\n",
    "        Test data matrix (m x p)\n",
    "    Y_test : ndarray, optional\n",
    "        Test target variable (m x 1 or m)\n",
    "    kernel_type : str\n",
    "        Type of kernel for L ('linear', 'rbf', 'delta')\n",
    "    sigma : float\n",
    "        RBF kernel bandwidth\n",
    "    max_iter : int\n",
    "        Maximum iterations for convergence\n",
    "    tol : float\n",
    "        Convergence tolerance\n",
    "    verbose : bool\n",
    "        If True, print iteration details\n",
    "    standardize : bool\n",
    "        If True, standardize X and Y\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        Contains:\n",
    "        - Z: Dimension reduced training data (n x K)\n",
    "        - V: Sparse eigenvectors (p x K)\n",
    "        - U: Left singular vectors (n x K)\n",
    "        - Lambda: Singular values (K,)\n",
    "        - Y_pred_train: Predicted Y for training data\n",
    "        - rmse_train: RMSE for training data (regression)\n",
    "        - Z_test: Dimension reduced test data (m x K), if X_test provided\n",
    "        - Y_pred_test: Predicted Y for test data, if X_test and Y_test provided\n",
    "        - rmse_test: RMSE for test data, if X_test and Y_test provided\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    n, p = X.shape\n",
    "    Y = Y.reshape(-1) if Y.ndim == 1 else Y[:, 0]\n",
    "\n",
    "    # Standardize data if requested\n",
    "    scaler_X, scaler_Y = None, None\n",
    "    if standardize:\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_Y = StandardScaler()\n",
    "        X = scaler_X.fit_transform(X)\n",
    "        Y = scaler_Y.fit_transform(Y.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    # Step 1: Compute kernel matrix\n",
    "    L = compute_kernel(Y, kernel_type, sigma)\n",
    "    if L.shape != (n, n):\n",
    "        raise ValueError(\"L must be n x n\")\n",
    "\n",
    "    # Step 2: Decompose L such that L = Delta^T Delta\n",
    "    U_L, S_L, Vt_L = np.linalg.svd(L, hermitian=True)\n",
    "    S_L = np.clip(S_L, 0, None)\n",
    "    Delta = U_L @ np.diag(np.sqrt(S_L))\n",
    "\n",
    "    # Step 3: Compute H = I - (1/n)e^T e\n",
    "    e = np.ones((n, 1))\n",
    "    H = np.eye(n) - (1/n) * (e @ e.T)\n",
    "\n",
    "    # Step 4: Compute Psi = Delta^T H X\n",
    "    Psi = Delta.T @ H @ X\n",
    "\n",
    "    # Step 5: Compute sparse basis using PMD\n",
    "    V = np.zeros((p, K))\n",
    "    U = np.zeros((n, K))\n",
    "    Lambda = np.zeros(K)\n",
    "    Psi_k = Psi.copy()\n",
    "\n",
    "    for k in range(K):\n",
    "        # Initialize v_k with L2-norm = 1\n",
    "        v_k = np.random.randn(p)\n",
    "        v_k = v_k / np.linalg.norm(v_k)\n",
    "\n",
    "        for iter_idx in range(max_iter):\n",
    "            v_old = v_k.copy()\n",
    "\n",
    "            # Update u_k\n",
    "            u_k = Psi_k @ v_k\n",
    "            if np.linalg.norm(u_k) < 1e-10:\n",
    "                print(f\"Warning: u_k norm too small at component {k+1}, iteration {iter_idx+1}\")\n",
    "                u_k = np.random.randn(n)\n",
    "\n",
    "            # Orthogonalize u_k against previous u_j\n",
    "            if k > 0:\n",
    "                u_k = u_k - U[:, :k] @ (U[:, :k].T @ u_k)\n",
    "                if np.linalg.norm(u_k) < 1e-10:\n",
    "                    print(f\"Warning: u_k norm too small after orthogonalization at component {k+1}\")\n",
    "                    u_k = np.random.randn(n)\n",
    "                u_k = u_k / np.linalg.norm(u_k)\n",
    "\n",
    "            # Update v_k\n",
    "            a = Psi_k.T @ u_k\n",
    "            tau = find_tau(a, c) \n",
    "            v_k = soft_threshold(a, tau)\n",
    "            if np.linalg.norm(v_k) < 1e-10:\n",
    "                print(f\"Warning: v_k norm too small at component {k+1}, iteration {iter_idx+1}\")\n",
    "                v_k = np.random.randn(p)\n",
    "            v_k = v_k / np.linalg.norm(v_k)\n",
    "\n",
    "            # Check convergence\n",
    "            norm_diff = np.linalg.norm(v_k - v_old)\n",
    "            if norm_diff < tol * np.linalg.norm(v_old):\n",
    "                if verbose:\n",
    "                    print(f\"Component {k+1} converged in {iter_idx+1} iterations\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Warning: PMD did not converge for component {k+1} after {max_iter} iterations\")\n",
    "\n",
    "        Lambda[k] = u_k.T @ Psi_k @ v_k\n",
    "        U[:, k] = u_k\n",
    "        V[:, k] = v_k\n",
    "\n",
    "        # Deflate Psi\n",
    "        Psi_k = Psi_k - Lambda[k] * np.outer(u_k, v_k)\n",
    "\n",
    "    # Step 6: Encode training data\n",
    "    Z = X @ V\n",
    "\n",
    "    # Step 7: Regression for evaluation (assuming regression task)\n",
    "    result = {\n",
    "        'Z': Z,\n",
    "        'V': V,\n",
    "        #'U': U,\n",
    "        #'Lambda': Lambda\n",
    "    }\n",
    "\n",
    "    if X_test is not None:\n",
    "        #X_test = scaler_X.fit_transform(X_test)\n",
    "        Z_test = X_test @ V\n",
    "        result['Z_test'] = Z_test\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 2)\n",
      "(7129, 70)\n",
      "(7129, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/0025j9bx74b7s62z1cxmw0ch0000gp/T/ipykernel_9627/1327986414.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Y = y.replace({'ALL':0,'AML':1})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "# Read data\n",
    "y = pd.read_csv(\"actual.csv\")\n",
    "data_2 = pd.read_csv(\"data_set_ALL_AML_independent.csv\")\n",
    "data_3 = pd.read_csv(\"data_set_ALL_AML_train.csv\")\n",
    "print(y.shape)\n",
    "print(data_2.shape)\n",
    "print(data_3.shape)\n",
    "\n",
    "Y = y.replace({'ALL':0,'AML':1})\n",
    "labels = ['ALL', 'AML']\n",
    "\n",
    "train_to_keep = [col for col in data_3.columns if \"call\" not in col]\n",
    "test_to_keep = [col for col in data_2.columns if \"call\" not in col]\n",
    "\n",
    "X_train_tr = data_3[train_to_keep]\n",
    "X_test_tr = data_2[test_to_keep]\n",
    "\n",
    "train_columns_titles = ['Gene Description', 'Gene Accession Number', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25',\n",
    "       '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38']\n",
    "\n",
    "X_train_tr = X_train_tr.reindex(columns=train_columns_titles)\n",
    "\n",
    "test_columns_titles = ['Gene Description', 'Gene Accession Number','39', '40', '41', '42', '43', '44', '45', '46',\n",
    "       '47', '48', '49', '50', '51', '52', '53',  '54', '55', '56', '57', '58', '59',\n",
    "       '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72']\n",
    "\n",
    "X_test_tr = X_test_tr.reindex(columns=test_columns_titles)\n",
    "\n",
    "X_train = X_train_tr.T\n",
    "X_test = X_test_tr.T\n",
    "\n",
    "X_train.columns = X_train.iloc[1]\n",
    "X_train = X_train.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n",
    "\n",
    "# Clean up the column names for Testing data\n",
    "X_test.columns = X_test.iloc[1]\n",
    "X_test = X_test.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "Y_train = Y[Y.patient <= 38].reset_index(drop=True)\n",
    "\n",
    "# Subset the rest for testing\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "Y_test = Y[Y.patient > 38].reset_index(drop=True)\n",
    "\n",
    "X_train_fl = X_train.astype(float, 64)\n",
    "X_test_fl = X_test.astype(float, 64)\n",
    "\n",
    "\n",
    "#X_train_fl = X_train_fl.iloc[:, :500]\n",
    "#X_test_fl = X_test_fl.iloc[:, :500]\n",
    "\n",
    "# Apply the same scaling to both datasets\n",
    "scaler = StandardScaler()\n",
    "X_train_scl = scaler.fit_transform(X_train_fl)\n",
    "X_test_scl = scaler.transform(X_test_fl)\n",
    "\n",
    "Y_train = Y_train['cancer']\n",
    "Y_test = Y_test['cancer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_methods_analysis_real_data(X_train, Y_train, X_test, Y_test, n_components_list=[2], threshold=0.25):\n",
    "    p = X_train.shape[1]\n",
    "    Y_train = np.array(Y_train).flatten()\n",
    "    Y_test = np.array(Y_test).flatten()\n",
    "\n",
    "    results = {\n",
    "        'n_components': n_components_list,\n",
    "        'precision_PCR': [],\n",
    "        'error_PCR': [],\n",
    "        'accuracy_PCR': [],\n",
    "        'non_zero_vars_PCR': [],\n",
    "        'auc_PCR': [],\n",
    "        'precision_LDA': [],\n",
    "        'error_LDA': [],\n",
    "        'accuracy_LDA': [],\n",
    "        'non_zero_vars_LDA': [],\n",
    "        'auc_LDA': [],\n",
    "        'precision_Barshan': [],\n",
    "        'error_Barshan': [],\n",
    "        'accuracy_Barshan': [],\n",
    "        'non_zero_vars_Barshan': [],\n",
    "        'auc_Barshan': [],\n",
    "        'precision_Bair': [],\n",
    "        'error_Bair': [],\n",
    "        'accuracy_Bair': [],\n",
    "        'non_zero_vars_Bair': [],\n",
    "        'auc_Bair': [],\n",
    "        'precision_Supervised': [],\n",
    "        'error_Supervised': [],\n",
    "        'accuracy_Supervised': [],\n",
    "        'non_zero_vars_Supervised': [],\n",
    "        'auc_Supervised': [],\n",
    "        'precision_Sparse': [],\n",
    "        'error_Sparse': [],\n",
    "        'accuracy_Sparse': [],\n",
    "        'non_zero_vars_Sparse': [],\n",
    "        'auc_Sparse': [],\n",
    "        'precision_SDSPCA': [],\n",
    "        'error_SDSPCA': [],\n",
    "        'accuracy_SDSPCA': [],\n",
    "        'non_zero_vars_SDSPCA': [],\n",
    "        'auc_SDSPCA': [],\n",
    "        #'precision_SPCA': [],\n",
    "        #'error_SPCA': [],\n",
    "        #'accuracy_SPCA': [],\n",
    "        #'non_zero_vars_SPCA': [],\n",
    "        #'auc_SPCA': [],\n",
    "        'precision_SSPCA': [],\n",
    "        'error_SSPCA': [],\n",
    "        'accuracy_SSPCA': [],\n",
    "        'non_zero_vars_SSPCA': [],\n",
    "        'auc_SSPCA': []\n",
    "    }\n",
    "\n",
    "    \n",
    "    #para_spca_grid = [[0.1, 0.1]]    #,[0.1,0.1],[1,1]]  \n",
    "    c_sspca_grid = [onp.sqrt(p)/32,onp.sqrt(p)/16,onp.sqrt(p)/8, onp.sqrt(p)/4, onp.sqrt(p)/2,]# onp.sqrt(p)]         \n",
    "\n",
    "    \n",
    "    X_train_main, X_val, Y_train_main, Y_val = train_test_split(\n",
    "        X_train, Y_train, test_size=0.2, random_state=2002)\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_main = scaler_X.fit_transform(X_train_main)\n",
    "    X_val = scaler_X.transform(X_val)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    \n",
    "    def count_nonzero_vars(W, threshold=1e-7):\n",
    "        if W.ndim == 1:\n",
    "            W = W.reshape(-1, 1)\n",
    "        return onp.mean(onp.sum(onp.abs(W) > threshold, axis=0))\n",
    "\n",
    "    for n_components in n_components_list:\n",
    "        # PCA\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_train_pca = pca.fit_transform(X_train_main)\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "        clf_pcr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        clf_pcr.fit(X_train_pca, Y_train_main)\n",
    "        Y_pred = clf_pcr.predict(X_test_pca)\n",
    "        Y_pred_proba = clf_pcr.predict_proba(X_test_pca)[:, 1]\n",
    "        results['precision_PCR'].append(precision_score(Y_test, Y_pred, zero_division = 0))\n",
    "        results['error_PCR'].append(1 - accuracy_score(Y_test, Y_pred))\n",
    "        results['accuracy_PCR'].append(accuracy_score(Y_test, Y_pred))\n",
    "        results['non_zero_vars_PCR'].append(p)\n",
    "        results['auc_PCR'].append(roc_auc_score(Y_test, Y_pred_proba))\n",
    "        \n",
    "        # LDA\n",
    "        n_comp_lda = min(n_components, len(np.unique(Y_train_main))-1, X_train_main.shape[1])\n",
    "        lda = LinearDiscriminantAnalysis(n_components=n_comp_lda)\n",
    "        X_train_lda = lda.fit_transform(X_train_main, Y_train_main)\n",
    "        X_test_lda = lda.transform(X_test_scaled)\n",
    "        Y_pred_lda = lda.predict(X_test_scaled)\n",
    "        Y_pred_proba_lda = lda.predict_proba(X_test_scaled)[:, 1]\n",
    "        results['precision_LDA'].append(precision_score(Y_test, Y_pred_lda, zero_division = 0))\n",
    "        results['error_LDA'].append(1 - accuracy_score(Y_test, Y_pred_lda))\n",
    "        results['accuracy_LDA'].append(accuracy_score(Y_test, Y_pred_lda))\n",
    "        results['non_zero_vars_LDA'].append(p)\n",
    "        results['auc_LDA'].append(roc_auc_score(Y_test,Y_pred_proba_lda))\n",
    "        \n",
    "        # HSIC\n",
    "        K = delta_kernel(Y_train_main)\n",
    "        spca_result = barshan_pca(X_train_main, Y_train_main, K, n_components)\n",
    "        W_barshan = spca_result['W']\n",
    "        Z_test_bar = X_test_scaled @ W_barshan\n",
    "        clf_barshan = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        clf_barshan.fit(X_train_main @ W_barshan, Y_train_main)\n",
    "        Y_pred_bar = clf_barshan.predict(Z_test_bar)\n",
    "        Y_pred_proba_bar = clf_barshan.predict_proba(Z_test_bar)[:, 1]\n",
    "        results['precision_Barshan'].append(precision_score(Y_test, Y_pred_bar, zero_division = 0))\n",
    "        results['error_Barshan'].append(1 - accuracy_score(Y_test, Y_pred_bar))\n",
    "        results['accuracy_Barshan'].append(accuracy_score(Y_test, Y_pred_bar))\n",
    "        results['non_zero_vars_Barshan'].append(count_nonzero_vars(W_barshan))\n",
    "        results['auc_Barshan'].append(roc_auc_score(Y_test,Y_pred_proba_bar))\n",
    "\n",
    "        # Bair's\n",
    "        feature_scores = onp.abs(onp.corrcoef(X_train_main.T, Y_train_main.T)[:-1, -1])\n",
    "        selected_features = feature_scores >= threshold\n",
    "        X_train_selected = X_train_main[:, selected_features]\n",
    "        X_test_selected = X_test_scaled[:, selected_features]\n",
    "        pca_bair = PCA(n_components=min(n_components, X_train_selected.shape[1]))\n",
    "        X_train_bair = pca_bair.fit_transform(X_train_selected)\n",
    "        X_test_bair = pca_bair.transform(X_test_selected)\n",
    "        clf_bair = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        clf_bair.fit(X_train_bair, Y_train_main)\n",
    "        Y_pred_bair = clf_bair.predict(X_test_bair)\n",
    "        Y_pred_proba_bair = clf_bair.predict_proba(X_test_bair)[:, 1]\n",
    "        results['precision_Bair'].append(precision_score(Y_test, Y_pred_bair, zero_division = 0))\n",
    "        results['error_Bair'].append(1 - accuracy_score(Y_test, Y_pred_bair))\n",
    "        results['accuracy_Bair'].append(accuracy_score(Y_test, Y_pred_bair))\n",
    "        results['non_zero_vars_Bair'].append(onp.sum(selected_features))\n",
    "        results['auc_Bair'].append(roc_auc_score(Y_test,Y_pred_proba_bair))\n",
    "\n",
    "        # CSPCA\n",
    "        lambda_grid = [0.01]\n",
    "        best_lambda = lambda_grid[0]\n",
    "        best_error = float('inf')\n",
    "        for lambda_ in lambda_grid:\n",
    "            spca_sup = supervised_pca(X_train_main, Y_train_main, n_components, p=p, m=145, lambda_=lambda_)\n",
    "            W_sup = spca_sup['W']\n",
    "            Z_val_sup = X_val @ W_sup\n",
    "            clf_sup = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "            clf_sup.fit(X_train_main @ W_sup, Y_train_main)\n",
    "            Y_pred_sup = clf_sup.predict(Z_val_sup)\n",
    "            current_error = 1 - accuracy_score(Y_val, Y_pred_sup)\n",
    "            if current_error < best_error:\n",
    "                best_error = current_error\n",
    "                best_lambda = lambda_\n",
    "        \n",
    "        spca_sup = supervised_pca(X_train_main, Y_train_main, n_components, p=p, m=145, lambda_=best_lambda)\n",
    "        W_sup = spca_sup['W']\n",
    "        Z_test_sup = X_test_scaled @ W_sup\n",
    "        clf_sup = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        clf_sup.fit(X_train_main @ W_sup, Y_train_main)\n",
    "        Y_pred_sup = clf_sup.predict(Z_test_sup)\n",
    "        Y_pred_proba_sup = clf_sup.predict_proba(Z_test_sup)[:, 1]\n",
    "        results['precision_Supervised'].append(precision_score(Y_test, Y_pred_sup, zero_division = 0))\n",
    "        results['error_Supervised'].append(1 - accuracy_score(Y_test, Y_pred_sup))\n",
    "        results['accuracy_Supervised'].append(accuracy_score(Y_test, Y_pred_sup))\n",
    "        results['non_zero_vars_Supervised'].append(count_nonzero_vars(W_sup))\n",
    "        results['auc_Supervised'].append(roc_auc_score(Y_test,Y_pred_proba_sup))\n",
    "        \n",
    "        # SCSPCA \n",
    "        eta_sparse = 10000  \n",
    "        kappa = 0.1\n",
    "        epsilon = 1e-6\n",
    "        n = Y_train_main.shape[0]\n",
    "        K = delta_kernel(Y_train_main) + epsilon * np.eye(n)\n",
    "        C = X_train_main.T @ K @ X_train_main + kappa * X_train_main.T @ X_train_main\n",
    "        W_manpg, F_manpg, sparsity, time_manpg, iter_, flag_succ, num_linesearch, mean_ssn = manpg_orth_sparse(\n",
    "            C, n_components, p, eta_sparse * onp.ones(p))\n",
    "        \n",
    "        X_test_proj = X_test_scaled @ W_manpg\n",
    "        clf_sparse = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        clf_sparse.fit(X_train_main @ W_manpg, Y_train_main)\n",
    "        Y_pred_sparse = clf_sparse.predict(X_test_proj)\n",
    "        Y_pred_proba_sparse = clf_sparse.predict_proba(X_test_proj)[:, 1]\n",
    "        results['precision_Sparse'].append(precision_score(Y_test, Y_pred_sparse, zero_division = 0))\n",
    "        results['error_Sparse'].append(1 - accuracy_score(Y_test, Y_pred_sparse))\n",
    "        results['accuracy_Sparse'].append(accuracy_score(Y_test, Y_pred_sparse))\n",
    "        results['non_zero_vars_Sparse'].append(count_nonzero_vars(W_manpg))\n",
    "        results['auc_Sparse'].append(roc_auc_score(Y_test,Y_pred_proba_sparse))\n",
    "\n",
    "        # SDSPCA\n",
    "        alpha_grid = [10000,1000000,10000000]\n",
    "        beta_grid = [0.0000000000000001,0.000000000000001,0.00000000000001,0.0000000000001,0.000000000001,0.00000000001]\n",
    "\n",
    "        \n",
    "        best_alpha = alpha_grid[0]\n",
    "        best_beta = beta_grid[0]\n",
    "        best_error = float('inf')\n",
    "        for alpha in alpha_grid:\n",
    "            for beta in beta_grid:\n",
    "                sdspca = SDSPCA(n_components=n_components, alpha=alpha, beta=beta)\n",
    "                Q_train = sdspca.fit_transform(X_train_main, Y_train_main)\n",
    "                Q_val = sdspca.transform(X_val)\n",
    "                clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "                clf.fit(Q_train, Y_train_main)\n",
    "                Y_pred = clf.predict(Q_val)\n",
    "                current_error = 1 - accuracy_score(Y_val, Y_pred)\n",
    "                if current_error < best_error:\n",
    "                    best_error = current_error\n",
    "                    best_alpha = alpha\n",
    "                    best_beta = beta       \n",
    "\n",
    "        print(f\"Best SDSPCA params: alpha={best_alpha}, beta={best_beta}\")\n",
    "        sdspca = SDSPCA(n_components=n_components, alpha=best_alpha, beta=best_beta)\n",
    "        Q_train = sdspca.fit_transform(X_train_main, Y_train_main)\n",
    "        Q_test = sdspca.transform(X_test_scaled)\n",
    "        clf_sdspca = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        clf_sdspca.fit(Q_train, Y_train_main)\n",
    "        Y_pred_sdspca = clf_sdspca.predict(Q_test)\n",
    "        Y_pred_proba_sdspca = clf_sdspca.predict_proba(Q_test)[:, 1]\n",
    "        print(sdspca.Q_)\n",
    "        results['precision_SDSPCA'].append(precision_score(Y_test, Y_pred_sdspca, zero_division = 0))\n",
    "        results['error_SDSPCA'].append(1 - accuracy_score(Y_test, Y_pred_sdspca))\n",
    "        results['accuracy_SDSPCA'].append(accuracy_score(Y_test, Y_pred_sdspca))\n",
    "        results['non_zero_vars_SDSPCA'].append(count_nonzero_vars(sdspca.Q_))\n",
    "        results['auc_SDSPCA'].append(roc_auc_score(Y_test,Y_pred_proba_sdspca))\n",
    "\n",
    "\n",
    "        # SPCA \n",
    "        #best_para_spca = para_spca_grid[0]\n",
    "        #best_error = float('inf')\n",
    "        #for para_spca in para_spca_grid:\n",
    "        #    spca_result = spca(X_train_main, K=n_components, \n",
    "        #         para=para_spca, \n",
    "        #         type_=\"predictor\", sparse=\"penalty\", lambda_=1e-4)\n",
    "        #    W_spca = spca_result['loadings']\n",
    "        #    Z_val_spca = X_val @ W_spca\n",
    "        #    clf_spca = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        #    clf_spca.fit(X_train_main @ W_spca, Y_train_main)\n",
    "        #    Y_pred_spca = clf_spca.predict(Z_val_spca)\n",
    "        #    current_error = 1 - accuracy_score(Y_val, Y_pred_spca)\n",
    "        #    if current_error < best_error:\n",
    "        #        best_error = current_error\n",
    "        #        best_para_spca = para_spca\n",
    "\n",
    "        #print(f\"Selected para_spca: {best_para_spca}\")\n",
    "        #spca_result = spca(X_train_main, K=n_components, \n",
    "        #                 para=best_para_spca, \n",
    "        #                 type_=\"predictor\", sparse=\"penalty\", lambda_=1e-4)\n",
    "        #W_spca = spca_result['loadings']\n",
    "        #Z_test_spca = X_test_scaled @ W_spca\n",
    "        #clf_spca = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        #clf_spca.fit(X_train_main @ W_spca, Y_train_main)\n",
    "        #Y_pred_spca = clf_spca.predict(Z_test_spca)\n",
    "        #Y_pred_proba_spca = clf_spca.predict_proba(Z_test_spca)[:, 1]\n",
    "        #results['precision_SPCA'].append(precision_score(Y_test, Y_pred_spca, zero_division = 0))\n",
    "        #results['error_SPCA'].append(1 - accuracy_score(Y_test, Y_pred_spca))\n",
    "        #results['accuracy_SPCA'].append(accuracy_score(Y_test, Y_pred_spca))\n",
    "        #results['non_zero_vars_SPCA'].append(count_nonzero_vars(W_spca))\n",
    "        #results['auc_SPCA'].append(roc_auc_score(Y_test,Y_pred_proba_spca))\n",
    "\n",
    "        # SSPCA\n",
    "        best_c_sspca = c_sspca_grid[0]  \n",
    "        best_error = float('inf')\n",
    "        for c_sspca in c_sspca_grid:\n",
    "            sspca_result = sspca(X_train_main, Y_train_main, K=n_components, c=c_sspca, \n",
    "                   X_test=X_val, Y_test=Y_val, kernel_type='delta')\n",
    "            Z_val_sspca = sspca_result['Z_test']\n",
    "            clf_sspca = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "            clf_sspca.fit(sspca_result['Z'], Y_train_main)\n",
    "            Y_pred_sspca = clf_sspca.predict(Z_val_sspca)\n",
    "            current_error = 1 - accuracy_score(Y_val, Y_pred_sspca)\n",
    "            if current_error < best_error:\n",
    "                best_error = current_error\n",
    "                best_c_sspca = c_sspca   \n",
    "        \n",
    "        print(f\"Selected c_sspca: {best_c_sspca}\")\n",
    "        sspca_result = sspca(X_train_main, Y_train_main, K=n_components, c=best_c_sspca, \n",
    "                           X_test=X_test_scaled, Y_test=Y_test, kernel_type='delta')\n",
    "        Z_test_sspca = sspca_result['Z_test']\n",
    "        W_sspca = sspca_result['V']\n",
    "        clf_sspca = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        clf_sspca.fit(sspca_result['Z'], Y_train_main)\n",
    "        Y_pred_sspca = clf_sspca.predict(Z_test_sspca)\n",
    "        Y_pred_proba_sspca = clf_sspca.predict_proba(Z_test_sspca)[:, 1]\n",
    "        results['precision_SSPCA'].append(precision_score(Y_test, Y_pred_sspca, zero_division = 0))\n",
    "        results['error_SSPCA'].append(1 - accuracy_score(Y_test, Y_pred_sspca))\n",
    "        results['accuracy_SSPCA'].append(accuracy_score(Y_test, Y_pred_sspca))\n",
    "        results['non_zero_vars_SSPCA'].append(count_nonzero_vars(W_sspca))\n",
    "        results['auc_SSPCA'].append(roc_auc_score(Y_test,Y_pred_proba_sspca))\n",
    "    \n",
    "    final_results = []\n",
    "    \n",
    "    for i, n_components in enumerate(n_components_list):\n",
    "        row = {'n_components': n_components}\n",
    "        \n",
    "        for method in ['PCR', 'LDA', 'Barshan', 'Bair' ,'Supervised' , 'Sparse' , 'SDSPCA', 'SSPCA']:\n",
    "            row[f'precision_{method}'] = results[f'precision_{method}'][i]\n",
    "            row[f'error_{method}'] = results[f'error_{method}'][i]\n",
    "            row[f'accuracy_{method}'] = results[f'accuracy_{method}'][i]\n",
    "            row[f'non_zero_vars_{method}'] = results[f'non_zero_vars_{method}'][i]\n",
    "            row[f'auc_{method}'] = results[f'auc_{method}'][i]\n",
    "        \n",
    "        final_results.append(row)\n",
    "    \n",
    "    return pd.DataFrame(final_results)\n",
    "\n",
    "def analyze_real_dataset(X_train, Y_train, X_test, Y_test, n_components_list=[2]):\n",
    "    results_df = run_all_methods_analysis_real_data(X_train, Y_train, X_test, Y_test, \n",
    "                                                  n_components_list=n_components_list)\n",
    "    \n",
    "    print(\"\\nReal Dataset Analysis Results:\")\n",
    "    for method in ['PCR', 'LDA', 'Barshan', 'Bair', 'Supervised', 'Sparse', 'SDSPCA', 'SSPCA']:\n",
    "        print(f\"\\nMethod: {method}\")\n",
    "        for n_comp in n_components_list:\n",
    "            row = results_df[results_df['n_components'] == n_comp].iloc[0]\n",
    "            print(f\"  n_components = {n_comp}:\")\n",
    "            print(f\"    Precision: {row[f'precision_{method}']:.4f}\")\n",
    "            print(f\"    Classification Error: {row[f'error_{method}']:.4f}\")\n",
    "            print(f\"    Accuracy: {row[f'accuracy_{method}']:.4f}\")\n",
    "            print(f\"    Non-zero variables: {row[f'non_zero_vars_{method}']:.1f}\")\n",
    "            print(f\"    AUC: {row[f'auc_{method}']:.1f}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ManPG: Iter 999, Fval 4.18760e+04, CPU 565.15, Sparsity 1.00, Inner Inexact 1994, Avg Inner 101.00, Total Linesearch 13930\n",
      "Best SDSPCA params: alpha=10000, beta=1e-16\n",
      "[[-0.19987946 -0.04594855]\n",
      " [ 0.01180976 -0.38354536]\n",
      " [-0.20180091 -0.02109379]\n",
      " [ 0.01757462 -0.42248296]\n",
      " [-0.20338734 -0.00951083]\n",
      " [-0.2099622   0.04356725]\n",
      " [-0.20195461 -0.02583698]\n",
      " [-0.20210213 -0.02468345]\n",
      " [-0.19992137 -0.04741524]\n",
      " [ 0.01302857 -0.39097052]\n",
      " [-0.20258055 -0.02569972]\n",
      " [ 0.01091105 -0.37819898]\n",
      " [-0.19977052 -0.0423807 ]\n",
      " [-0.21173572  0.05279027]\n",
      " [-0.20861258  0.03084514]\n",
      " [-0.20326421 -0.01443573]\n",
      " [-0.19733531 -0.06611467]\n",
      " [-0.20349926 -0.01254665]\n",
      " [ 0.02261012 -0.46611671]\n",
      " [-0.20349487 -0.00604134]\n",
      " [ 0.01051344 -0.37533103]\n",
      " [-0.20558916  0.00577241]\n",
      " [-0.20477228  0.00192451]\n",
      " [-0.20848521  0.03091728]\n",
      " [-0.20569598  0.00713842]\n",
      " [-0.20496435  0.00084429]\n",
      " [-0.20378624 -0.00596562]\n",
      " [-0.20328915 -0.01501836]\n",
      " [-0.20492829  0.0049116 ]\n",
      " [-0.20420782 -0.00160603]]\n",
      "Warning: PMD did not converge for component 2 after 1000 iterations\n",
      "Warning: PMD did not converge for component 2 after 1000 iterations\n",
      "Warning: PMD did not converge for component 2 after 1000 iterations\n",
      "Warning: PMD did not converge for component 2 after 1000 iterations\n",
      "Warning: PMD did not converge for component 2 after 1000 iterations\n",
      "Selected c_sspca: 2.638543928476462\n",
      "Warning: PMD did not converge for component 2 after 1000 iterations\n",
      "\n",
      "Real Dataset Analysis Results:\n",
      "\n",
      "Method: PCR\n",
      "  n_components = 2:\n",
      "    Precision: 0.0000\n",
      "    Classification Error: 0.4412\n",
      "    Accuracy: 0.5588\n",
      "    Non-zero variables: 7129.0\n",
      "    AUC: 0.8\n",
      "\n",
      "Method: LDA\n",
      "  n_components = 2:\n",
      "    Precision: 1.0000\n",
      "    Classification Error: 0.3824\n",
      "    Accuracy: 0.6176\n",
      "    Non-zero variables: 7129.0\n",
      "    AUC: 0.9\n",
      "\n",
      "Method: Barshan\n",
      "  n_components = 2:\n",
      "    Precision: 1.0000\n",
      "    Classification Error: 0.2059\n",
      "    Accuracy: 0.7941\n",
      "    Non-zero variables: 7128.0\n",
      "    AUC: 1.0\n",
      "\n",
      "Method: Bair\n",
      "  n_components = 2:\n",
      "    Precision: 1.0000\n",
      "    Classification Error: 0.2647\n",
      "    Accuracy: 0.7353\n",
      "    Non-zero variables: 2615.0\n",
      "    AUC: 0.9\n",
      "\n",
      "Method: Supervised\n",
      "  n_components = 2:\n",
      "    Precision: 1.0000\n",
      "    Classification Error: 0.2059\n",
      "    Accuracy: 0.7941\n",
      "    Non-zero variables: 7129.0\n",
      "    AUC: 1.0\n",
      "\n",
      "Method: Sparse\n",
      "  n_components = 2:\n",
      "    Precision: 1.0000\n",
      "    Classification Error: 0.0294\n",
      "    Accuracy: 0.9706\n",
      "    Non-zero variables: 15.0\n",
      "    AUC: 1.0\n",
      "\n",
      "Method: SDSPCA\n",
      "  n_components = 2:\n",
      "    Precision: 0.7143\n",
      "    Classification Error: 0.2353\n",
      "    Accuracy: 0.7647\n",
      "    Non-zero variables: 30.0\n",
      "    AUC: 0.8\n",
      "\n",
      "Method: SSPCA\n",
      "  n_components = 2:\n",
      "    Precision: 1.0000\n",
      "    Classification Error: 0.1176\n",
      "    Accuracy: 0.8824\n",
      "    Non-zero variables: 36.0\n",
      "    AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "results = analyze_real_dataset(X_train_scl, Y_train, X_test_scl, Y_test, n_components_list=[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
